#!/usr/bin/env python3
"""
å¤šæº AI æ–°é—»æŠ“å–è„šæœ¬ - All-in-One
æ”¯æŒ 6 ä¸ªä¸»è¦ AI æ–°é—»æºï¼š
- TechCrunch AI
- VentureBeat AI
- The Verge AI
- MIT Technology Review
- AI News
- æœºå™¨ä¹‹å¿ƒ
"""

import feedparser
import requests
from datetime import datetime, timedelta
import os
import re
from typing import List, Dict, Any

# é…ç½®
NEWS_SOURCES = {
    'techcrunch': {
        'name': 'TechCrunch AI',
        'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'limit': 10,
        'enabled': True
    },
    'venturebeat': {
        'name': 'VentureBeat AI',
        'url': 'https://venturebeat.com/category/ai/feed/',
        'limit': 8,
        'enabled': True
    },
    'theverge': {
        'name': 'The Verge AI',
        'url': 'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
        'limit': 6,
        'enabled': True
    },
    'mit': {
        'name': 'MIT Technology Review',
        'url': 'https://www.technologyreview.com/topnews.rss',
        'limit': 5,
        'enabled': True,
        'filter': 'ai'
    },
    'ainews': {
        'name': 'AI News',
        'url': 'https://artificialintelligence-news.com/feed/',
        'limit': 8,
        'enabled': True
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'url': 'https://www.jiqizhixin.com/rss',
        'limit': 8,
        'enabled': True
    }
}

def clean_html(text):
    """æ¸…ç† HTML æ ‡ç­¾"""
    if not text:
        return ''
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&amp;', '&')
    text = text.replace('&#8217;', "'")
    text = text.replace('&#8216;', "'")
    text = text.replace('&#8220;', '"')
    text = text.replace('&#8221;', '"')
    text = text.replace('&#8230;', '...')
    text = text.replace('&quot;', '"')
    return text.strip()

def fetch_rss_feed(source_key: str, source_config: Dict) -> List[Dict[str, Any]]:
    """æŠ“å–å•ä¸ª RSS feed"""
    try:
        print(f"ğŸ“¡ æ­£åœ¨æŠ“å– {source_config['name']}...")
        feed = feedparser.parse(source_config['url'])
        entries = feed.entries[:source_config['limit']]
        
        articles = []
        for entry in entries:
            # AI ç›¸å…³æ€§è¿‡æ»¤
            title = entry.get('title', '').lower()
            description = entry.get('description', '').lower()
            
            # AI å…³é”®è¯æ£€æŸ¥
            ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                          'deep learning', 'llm', 'gpt', 'claude', 'gemini',
                          'neural network', 'robot', 'automation', 'agent']
            
            if not any(keyword in title or keyword in description for keyword in ai_keywords):
                # MIT éœ€è¦ç‰¹åˆ«å¤„ç†ï¼Œå› ä¸ºå®ƒä¸æ˜¯çº¯ AI æº
                if source_key == 'mit' and 'ai' not in title and 'artificial intelligence' not in description:
                    continue
                # é MIT æºï¼Œå¦‚æœæ²¡æœ‰ AI å…³é”®è¯å°±è·³è¿‡
                if source_key != 'mit':
                    continue
            
            article = {
                'title': entry.get('title', ''),
                'link': entry.get('link', ''),
                'description': clean_html(entry.get('description', '')),
                'published': entry.get('published', datetime.now().strftime('%a, %d %b %Y %H:%M:%S %z')),
                'author': entry.get('author', source_config['name']),
                'source': source_config['name']
            }
            articles.append(article)
        
        print(f"  âœ… è·å– {len(articles)} æ¡æ–°é—»")
        return articles
        
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def deduplicate_articles(all_articles: List[Dict]) -> List[Dict]:
    """å»é‡ - åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦"""
    seen = set()
    unique_articles = []
    
    for article in all_articles:
        # ä½¿ç”¨æ ‡é¢˜çš„å‰ 50 ä¸ªå­—ç¬¦ä½œä¸ºå»é‡ä¾æ®
        title_key = article['title'][:50].lower().strip()
        if title_key not in seen:
            seen.add(title_key)
            unique_articles.append(article)
    
    return unique_articles

def sort_by_recency(articles: List[Dict]) -> List[Dict]:
    """æŒ‰å‘å¸ƒæ—¶é—´æ’åº"""
    def parse_date(date_str):
        try:
            from dateutil import parser
            return parser.parse(date_str)
        except:
            return datetime.min
    
    return sorted(articles, key=lambda x: parse_date(x['published']), reverse=True)

def fetch_all_news():
    """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
    all_articles = []
    
    # è·å–ç¯å¢ƒå˜é‡
    news_dir = os.environ.get('NEWS_DIR', '/data1/cc/vide-coding/ai-news-hub/docs/news')
    date = os.environ.get('DATE', datetime.now().strftime('%Y-%m-%d'))
    datetime_str = os.environ.get('DATETIME', datetime.now().strftime('%Y-%m-%d %H:%M'))
    output_file = os.path.join(news_dir, f"{date}.md")
    
    print(f"\n{'='*60}")
    print(f"  AI æ–°é—»å¤šæºæŠ“å–å¼€å§‹ï¼ˆAll-in-One ç‰ˆï¼‰")
    print(f"{'='*60}")
    print(f"ğŸ“… æ—¥æœŸï¼š{date}")
    print(f"ğŸ• æ—¶é—´ï¼š{datetime_str}")
    print(f"{'='*60}\n")
    
    # æŠ“å–å„ä¸ªæº
    for source_key, config in NEWS_SOURCES.items():
        if config.get('enabled', True):
            articles = fetch_rss_feed(source_key, config)
            all_articles.extend(articles)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ€»è®¡è·å– {len(all_articles)} æ¡æ–°é—»")
    print(f"{'='*60}")
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_articles)} æ¡")
    
    # æ’åº
    sorted_articles = sort_by_recency(unique_articles)
    
    # é€‰æ‹©å‰ 20 æ¡æœ€æ–°æ–°é—»
    top_articles = sorted_articles[:20]
    
    # ç”Ÿæˆ Markdown
    content = generate_markdown(top_articles, date, datetime_str)
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nâœ… å·²ä¿å­˜ {len(top_articles)} æ¡æ–°é—»åˆ° {output_file}")
    print(f"{'='*60}\n")

def generate_markdown(articles: List[Dict], date: str, datetime_str: str) -> str:
    """ç”Ÿæˆ Markdown å†…å®¹"""
    content = f"""# {datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')} AI æ–°é—»ç®€æŠ¥ï¼ˆå¤šæºç‰ˆï¼‰

**æ¥æºï¼š** TechCrunch + VentureBeat + The Verge + MIT + AI News + æœºå™¨ä¹‹å¿ƒ
**æ›´æ–°æ—¶é—´ï¼š** {datetime_str}
**æ–°é—»æ•°é‡ï¼š** {len(articles)} æ¡

---

## ğŸ”¥ ä»Šæ—¥å¤´æ¡ï¼ˆTOP 5ï¼‰

"""
    
    # å‰ 5 æ¡ä½œä¸ºå¤´æ¡
    for i, article in enumerate(articles[:5], 1):
        content += format_article(article, i, detailed=True)
        content += "\n"
    
    # å…¶ä½™æ–°é—»
    if len(articles) > 5:
        content += "\n## ğŸ“° æ›´å¤šèµ„è®¯\n\n"
        for i, article in enumerate(articles[5:], 6):
            content += format_article(article, i, detailed=False)
            content += "\n"
    
    # æ•°æ®ç»Ÿè®¡
    source_counts = {}
    for article in articles:
        source = article['source']
        source_counts[source] = source_counts.get(source, 0) + 1
    
    content += "\n## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n"
    content += "**æ–°é—»æ¥æºåˆ†å¸ƒï¼š**\n\n"
    for source, count in source_counts.items():
        content += f"- {source}: {count} æ¡\n"
    
    content += f"""
---

**æ‰€æœ‰æ–°é—»æŒ‰å‘å¸ƒæ—¶é—´æ’åº**
**æ•°æ®æ¥æºï¼š** 6 ä¸ªä¸»è¦ AI æ–°é—»æº
**æ›´æ–°é¢‘ç‡ï¼š** æ¯æ—¥ 02:00 å’Œ 14:00

---

*è‡ªåŠ¨ç”Ÿæˆï¼šAI News Bot (All-in-One) | æœ€åæ›´æ–°ï¼š{datetime_str}*
"""
    
    return content

def format_article(article: Dict, index: int, detailed: bool = True) -> str:
    """æ ¼å¼åŒ–å•ç¯‡æ–‡ç« """
    formatted = f"""### {index}. {article['title']}

**ğŸ“… å‘å¸ƒæ—¶é—´ï¼š** {article['published']}
**âœï¸ æ¥æºï¼š** {article['source']}
**ğŸ”— åŸæ–‡é“¾æ¥ï¼š** [{article['title']}]({article['link']})
"""
    
    if detailed:
        description = article['description']
        if len(description) > 400:
            description = description[:400] + '...'
        formatted += f"""
**ğŸ“ å†…å®¹æ‘˜è¦ï¼š**
{description}

"""
    else:
        description = article['description']
        if len(description) > 150:
            description = description[:150] + '...'
        if description:
            formatted += f"""
{description}

"""
        else:
            formatted += "\n"
    
    formatted += "---\n"
    return formatted

if __name__ == '__main__':
    fetch_all_news()
