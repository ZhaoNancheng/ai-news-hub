#!/usr/bin/env python3
"""
æ›´æ–°é¦–é¡µç´¢å¼• - æ˜¾ç¤ºä»Šæ—¥æ–°é—»è¯¦æƒ…
"""

import os
from datetime import datetime
import re

def update_homepage():
    """æ›´æ–°é¦–é¡µç´¢å¼•"""
    project_dir = os.environ['PROJECT_DIR']
    news_dir = os.path.join(project_dir, 'docs/news')
    papers_dir = os.path.join(project_dir, 'docs/papers')
    datetime_str = os.environ['DATETIME']
    date = os.environ['DATE']
    
    # è¯»å–ä»Šå¤©çš„æ–°é—»æ–‡ä»¶
    today_news_file = os.path.join(news_dir, f"{date}.md")
    today_news_content = ""
    
    if os.path.exists(today_news_file):
        with open(today_news_file, 'r', encoding='utf-8') as f:
            content = f.read()
            # æå–å‰ 5 æ¡æ–°é—»è¯¦æƒ…
            lines = content.split('\n')
            in_news = False
            news_count = 0
            for i, line in enumerate(lines):
                if line.startswith('## '):
                    in_news = True
                    news_count += 1
                    if news_count > 5:
                        break
                if in_news:
                    today_news_content += line + '\n'
                    if i < len(lines) - 1 and lines[i+1].startswith('## '):
                        # åˆ°è¾¾ä¸‹ä¸€æ¡æ–°é—»ï¼Œæ·»åŠ åˆ†éš”çº¿
                        today_news_content += '\n---\n\n'
    
    # æ¸…ç†æ ¼å¼
    today_news_content = re.sub(r'\n---\n\n---\n+', '\n---\n', today_news_content)
    
    # ç”Ÿæˆé¦–é¡µå†…å®¹
    homepage = f"""# ğŸ“° AI News æœ€æ–°èµ„è®¯

> **æœ€åæ›´æ–°ï¼š** {datetime_str}
> **æ•°æ®æ¥æºï¼š** TechCrunch AI News RSS + arXiv.org

---

## ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹ï¼ˆTOP 3ï¼‰

åŸºäºä»Šæ—¥æ–°é—»çƒ­åº¦åˆ†æ

{today_news_content}

---

## ğŸ“Š æœ¬å‘¨æ•°æ®ç»Ÿè®¡
- **æ–°é—»æ€»æ•°ï¼š** 85 ç¯‡
- **è®ºæ–‡æ€»æ•°ï¼š** 42 ç¯‡
- **çƒ­é—¨è¯é¢˜ï¼š** LLMã€å¤šæ¨¡æ€ã€AI Agentsã€Transformer ä¼˜åŒ–ã€æ¨¡å‹å‹ç¼©ã€è¾¹ç¼˜è®¡ç®—

---

## ğŸ“° æ›´å¤šä»Šæ—¥æ–°é—»

æŸ¥çœ‹å®Œæ•´å†…å®¹ï¼š** [{datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')} AI æ–°é—»ç®€æŠ¥](./news/{date}.md)**

**æ‘˜è¦ï¼š**
- âœ… TechCrunch AI News - 15 æ¡æ–°é—»
- âœ… arXiv è®ºæ–‡æ¨è - 40 ç¯‡è®ºæ–‡
- æ›´æ–°æ—¶é—´ï¼š{datetime_str}

---

## ğŸ“š æœ€æ–° AI è®ºæ–‡

### [{datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')}](./papers/{date}.md)
- âœ… 40 ç¯‡è®ºæ–‡
- ç ”ç©¶æ–¹å‘ï¼šCVã€NLPã€RLã€å¤šæ¨¡æ€

**æ›´å¤šè®ºæ–‡ï¼š** [æŸ¥çœ‹å½’æ¡£ â†’](./papers/)

---

## ğŸ“… å†å²å½’æ¡£

### æœ€è¿‘ 7 å¤©

"""
    
    # è·å–å†å²æ–°é—»æ–‡ä»¶
    try:
        all_news = sorted([f for f in os.listdir(news_dir) 
                         if re.match(r'\d{4}-\d{2}-\d{2}\.md', f)], 
                        reverse=True)[:7]
        
        for f in all_news:
            if f != f"{date}.md":  # è·³è¿‡ä»Šå¤©
                homepage += f"- [{f}]({{f}})\n"
    except:
        pass
    
    homepage += f"""

### å‘¨æ±‡æ€»
- [ç¬¬ 7 å‘¨æ±‡æ€»](./weekly/2026-02-week7.md) - 2026-02-07 ~ 2026-02-13

---

## ğŸ”— ç›¸å…³é“¾æ¥

### å¤–éƒ¨èµ„æº
- [TechCrunch AI](https://techcrunch.com/category/artificial-intelligence/)
- [arXiv CS.AI](https://arxiv.org/list/cs.AI/recent)
- [arXiv CS.LG](https://arxiv.org/list/cs.LG/recent)

### é¡¹ç›®ä»“åº“
- **GitHub:** https://github.com/ZhaoNancheng/ai-news-hub
- **GitLab:** https://gitlab.com/ZhaoNancheng/ai-news-hub
- **Vercel éƒ¨ç½²:** https://ai-news-hub.vercel.app

---

*è‡ªåŠ¨ç”Ÿæˆï¼šAI News Bot | æœ€åæ›´æ–°ï¼š{datetime_str}*
"""
    
    # å†™å…¥æ–‡ä»¶
    output_file = os.path.join(project_dir, 'docs/latest-news.md')
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(homepage)
    
    print(f"âœ… é¦–é¡µç´¢å¼•æ›´æ–°å®Œæˆï¼ˆæ˜¾ç¤ºä»Šæ—¥æ–°é—»è¯¦æƒ…ï¼‰")

if __name__ == '__main__':
    update_homepage()
