#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå¤šæºæŠ“å– - é›†æˆåˆ†ç±»å’Œè¯„åˆ†ç³»ç»Ÿ
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__)))

# å¯¼å…¥åˆ†ç±»å’Œè¯„åˆ†å‡½æ•°
exec(open('/data1/cc/vide-coding/ai-news-hub/scripts/content-classifier.py').read())
import feedparser
import requests
from datetime import datetime, timedelta
import os
import re
from typing import List, Dict, Any

# é…ç½®
NEWS_SOURCES = {
    'techcrunch': {
        'name': 'TechCrunch AI',
        'url': 'https://techcrunch.com/category/artificial-intelligence/feed/',
        'limit': 10,
        'enabled': True
    },
    'venturebeat': {
        'name': 'VentureBeat AI',
        'url': 'https://venturebeat.com/category/ai/feed/',
        'limit': 8,
        'enabled': True
    },
    'theverge': {
        'name': 'The Verge AI',
        'url': 'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
        'limit': 6,
        'enabled': True
    },
    'mit': {
        'name': 'MIT Technology Review',
        'url': 'https://www.technologyreview.com/topnews.rss',
        'limit': 5,
        'enabled': True,
        'filter': 'ai'
    },
    'ainews': {
        'name': 'AI News',
        'url': 'https://artificialintelligence-news.com/feed/',
        'limit': 8,
        'enabled': True
    },
    'jiqizhixin': {
        'name': 'æœºå™¨ä¹‹å¿ƒ',
        'url': 'https://www.jiqizhixin.com/rss',
        'limit': 8,
        'enabled': True
    }
}

def clean_html(text):
    """æ¸…ç† HTML æ ‡ç­¾"""
    if not text:
        return ''
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&amp;', '&')
    text = text.replace('&#8217;', "'")
    text = text.replace('&#8216;', "'")
    text = text.replace('&#8220;', '"')
    text = text.replace('&#8221;', '"')
    text = text.replace('&#8230;', '...')
    text = text.replace('&quot;', '"')
    return text.strip()

def fetch_rss_feed(source_key: str, source_config: Dict) -> List[Dict[str, Any]]:
    """æŠ“å–å•ä¸ª RSS feed"""
    try:
        print(f"ğŸ“¡ æ­£åœ¨æŠ“å– {source_config['name']}...")
        feed = feedparser.parse(source_config['url'])
        entries = feed.entries[:source_config['limit']]
        
        articles = []
        for entry in entries:
            # AI ç›¸å…³æ€§è¿‡æ»¤
            title = entry.get('title', '').lower()
            description = entry.get('description', '').lower()
            
            # AI å…³é”®è¯æ£€æŸ¥
            ai_keywords = ['ai', 'artificial intelligence', 'machine learning', 
                          'deep learning', 'llm', 'gpt', 'claude', 'gemini',
                          'neural network', 'robot', 'automation', 'agent']
            
            if not any(keyword in title or keyword in description for keyword in ai_keywords):
                if source_key == 'mit' and 'ai' not in title and 'artificial intelligence' not in description:
                    continue
                if source_key != 'mit':
                    continue
            
            # æ¸…ç† HTML
            clean_desc = clean_html(entry.get('description', ''))
            
            article = {
                'title': entry.get('title', ''),
                'link': entry.get('link', ''),
                'description': clean_desc,
                'published': entry.get('published', datetime.now().strftime('%a, %d %b %Y %H:%M:%S %z')),
                'author': entry.get('author', source_config['name']),
                'source': source_config['name']
            }
            articles.append(article)
        
        print(f"  âœ… è·å– {len(articles)} æ¡æ–°é—»")
        return articles
        
    except Exception as e:
        print(f"  âŒ æŠ“å–å¤±è´¥: {e}")
        return []

def deduplicate_articles(all_articles: List[Dict]) -> List[Dict]:
    """å»é‡ - åŸºäºæ ‡é¢˜ç›¸ä¼¼åº¦"""
    seen = set()
    unique_articles = []
    
    for article in all_articles:
        # ä½¿ç”¨æ ‡é¢˜çš„å‰ 50 ä¸ªå­—ç¬¦ä½œä¸ºå»é‡ä¾æ®
        title_key = article['title'][:50].lower().strip()
        if title_key not in seen:
            seen.add(title_key)
            unique_articles.append(article)
    
    return unique_articles

def sort_by_priority(articles: List[Dict]) -> List[Dict]:
    """æŒ‰ä¼˜å…ˆçº§è¯„åˆ†æ’åº"""
    scored_articles = []
    
    for article in articles:
        score = calculate_priority_score(
            article['title'],
            article.get('description', ''),
            article['source']
        )
        article['priority_score'] = score
        scored_articles.append(article)
    
    # æŒ‰è¯„åˆ†é™åºæ’åº
    return sorted(scored_articles, key=lambda x: x['priority_score'], reverse=True)

def fetch_all_news():
    """æŠ“å–æ‰€æœ‰æ–°é—»æº"""
    all_articles = []
    
    # è·å–ç¯å¢ƒå˜é‡
    news_dir = os.environ.get('NEWS_DIR', '/data1/cc/vide-coding/ai-news-hub/docs/news')
    date = os.environ.get('DATE', datetime.now().strftime('%Y-%m-%d'))
    datetime_str = os.environ.get('DATETIME', datetime.now().strftime('%Y-%m-%d %H:%M'))
    output_file = os.path.join(news_dir, f"{date}.md")
    
    print(f"\n{'='*60}")
    print(f"  AI æ–°é—»å¤šæºæŠ“å–å¼€å§‹ï¼ˆEnhanced ç‰ˆï¼‰")
    print(f"{'='*60}")
    print(f"ğŸ“… æ—¥æœŸï¼š{date}")
    print(f"ğŸ• æ—¶é—´ï¼š{datetime_str}")
    print(f"{'='*60}\n")
    
    # æŠ“å–å„ä¸ªæº
    for source_key, config in NEWS_SOURCES.items():
        if config.get('enabled', True):
            articles = fetch_rss_feed(source_key, config)
            all_articles.extend(articles)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ€»è®¡è·å– {len(all_articles)} æ¡æ–°é—»")
    print(f"{'='*60}")
    
    # å»é‡
    unique_articles = deduplicate_articles(all_articles)
    print(f"ğŸ”„ å»é‡åå‰©ä½™ {len(unique_articles)} æ¡")
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    prioritized_articles = sort_by_priority(unique_articles)
    
    # é€‰æ‹©å‰ 25 æ¡é«˜è´¨é‡æ–°é—»
    top_articles = prioritized_articles[:25]
    
    # ç”Ÿæˆ Markdown
    content = generate_markdown_enhanced(top_articles, date, datetime_str)
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"\nâœ… å·²ä¿å­˜ {len(top_articles)} æ¡æ–°é—»åˆ° {output_file}")
    print(f"{'='*60}\n")

def generate_summary(articles: List[Dict], top_n: int = 5) -> str:
    """
    ç”Ÿæˆæ¯æ—¥çƒ­ç‚¹æ€»ç»“
    """
    from collections import Counter
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    scored_articles = sorted(articles, key=lambda x: x.get('priority_score', 0), reverse=True)
    
    # é€‰æ‹© top N
    top_articles = scored_articles[:top_n]
    
    # æå–å…³é”®ä¸»é¢˜
    all_tags = []
    for article in top_articles:
        categories, _ = classify_content(article['title'], article.get('description', ''))
        all_tags.extend(categories)
    
    # ç»Ÿè®¡æœ€å¸¸è§ä¸»é¢˜
    top_topics = [tag for tag, count in Counter(all_tags).most_common(5)]
    
    # ç”Ÿæˆæ€»ç»“
    summary = f"""## ğŸ”¥ ä»Šæ—¥çƒ­ç‚¹æ€»ç»“

**æœ€çƒ­é—¨ä¸»é¢˜ï¼š** {' â€¢ '.join(top_topics[:3])}

**ä»Šæ—¥ {top_n} å¤§äº‹ä»¶ï¼š**

"""
    
    for i, article in enumerate(top_articles, 1):
        title = article.get('title', '')
        source = article.get('source', '')
        score = article.get('priority_score', 0)
        
        summary += f"""### {i}. {title}
**æ¥æºï¼š** {source} â€¢ **ä¼˜å…ˆçº§ï¼š** {score:.1f}/100

"""
    
    return summary

def generate_markdown_enhanced(articles: List[Dict], date: str, datetime_str: str) -> str:
    """ç”Ÿæˆå¢å¼ºç‰ˆ Markdown å†…å®¹"""
    
    # ç”Ÿæˆä»Šæ—¥æ€»ç»“
    summary = generate_summary(articles, top_n=3)
    
    content = f"""# {datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')} AI æ–°é—»ç®€æŠ¥ï¼ˆå¢å¼ºç‰ˆï¼‰

**æ¥æºï¼š** TechCrunch + VentureBeat + The Verge + MIT + AI News + æœºå™¨ä¹‹å¿ƒ
**æ›´æ–°æ—¶é—´ï¼š** {datetime_str}
**æ–°é—»æ•°é‡ï¼š** {len(articles)} æ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

---

{summary}

---

## ğŸ”¥ ä»Šæ—¥å¤´æ¡ï¼ˆTOP 10ï¼‰

"""
    
    # å‰ 10 æ¡è¯¦ç»†å±•ç¤º
    for i, article in enumerate(articles[:10], 1):
        content += format_article_enhanced(article, i, detailed=True)
        content += "\n"
    
    # å…¶ä½™æ–°é—»
    if len(articles) > 10:
        content += "\n## ğŸ“° æ›´å¤šèµ„è®¯\n\n"
        for i, article in enumerate(articles[10:], 11):
            content += format_article_enhanced(article, i, detailed=False)
            content += "\n"
    
    # æ•°æ®ç»Ÿè®¡
    source_counts = {}
    category_counts = {}
    
    for article in articles:
        source = article['source']
        source_counts[source] = source_counts.get(source, 0) + 1
        
        # ç»Ÿè®¡åˆ†ç±»
        categories, _ = classify_content(article['title'], article.get('description', ''))
        for cat in categories:
            category_counts[cat] = category_counts.get(cat, 0) + 1
    
    content += "\n## ğŸ“Š æ•°æ®ç»Ÿè®¡\n\n"
    content += "**æ–°é—»æ¥æºåˆ†å¸ƒï¼š**\n\n"
    for source, count in sorted(source_counts.items(), key=lambda x: x[1], reverse=True):
        content += f"- {source}: {count} æ¡\n"
    
    content += "\n**çƒ­é—¨ä¸»é¢˜åˆ†å¸ƒï¼š**\n\n"
    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    for cat, count in top_categories:
        content += f"- {cat}: {count} æ¡\n"
    
    content += f"""
---

**æ‰€æœ‰æ–°é—»æŒ‰ä¼˜å…ˆçº§è¯„åˆ†æ’åºï¼ˆæ—¶æ•ˆæ€§+æ¥æºè´¨é‡+å†…å®¹å®Œæ•´æ€§+ç›¸å…³æ€§ï¼‰**
**æ•°æ®æ¥æºï¼š** 6 ä¸ªä¸»è¦ AI æ–°é—»æº
**æ›´æ–°é¢‘ç‡ï¼š** æ¯æ—¥ 02:00 å’Œ 14:00
**æ™ºèƒ½åŠŸèƒ½ï¼š** è‡ªåŠ¨åˆ†ç±»ã€æ ‡ç­¾ã€è´¨é‡è¯„åˆ†

---

*è‡ªåŠ¨ç”Ÿæˆï¼šAI News Bot (Enhanced) | æœ€åæ›´æ–°ï¼š{datetime_str}*
"""
    
    return content

def format_article_enhanced(article: Dict, index: int, detailed: bool = True) -> str:
    """æ ¼å¼åŒ–å•ç¯‡æ–‡ç« ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    
    # æ™ºèƒ½åˆ†ç±»
    categories, tags = classify_content(article['title'], article.get('description', ''))
    category_str = ' | '.join(categories[:2])
    tags_str = ' '.join(tags[:4])
    
    priority_score = article.get('priority_score', 0)
    
    formatted = f"""### {index}. {article['title']}

**ğŸ“Š ä¼˜å…ˆçº§ï¼š** {priority_score:.1f}/100
**ğŸ·ï¸ åˆ†ç±»ï¼š** {category_str}
**ğŸ“š æ ‡ç­¾ï¼š** {tags_str}
**ğŸ“… å‘å¸ƒæ—¶é—´ï¼š** {article['published']}
**âœï¸ æ¥æºï¼š** {article['source']}
**ğŸ”— åŸæ–‡é“¾æ¥ï¼š** [{article['title']}]({article['link']})
"""
    
    if detailed:
        description = article.get('description', '')
        if description and len(description) > 400:
            description = description[:400] + '...'
        if description:
            formatted += f"""
**ğŸ“ å†…å®¹æ‘˜è¦ï¼š**
{description}

"""
        else:
            formatted += "\n"
    else:
        description = article.get('description', '')
        if description and len(description) > 150:
            description = description[:150] + '...'
        if description:
            formatted += f"""
{description}

"""
        else:
            formatted += "\n"
    
    formatted += "---\n"
    return formatted

if __name__ == '__main__':
    fetch_all_news()
