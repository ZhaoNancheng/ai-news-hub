#!/usr/bin/env python3
"""
ç¤¾åŒºå¹³å°æŠ“å–è„šæœ¬ - Hacker News + GitHub Trending
ä¸éœ€è¦ VPNï¼Œä½¿ç”¨å…¬å¼€ API
"""

import requests
import json
from datetime import datetime
import os
from typing import List, Dict, Any

# é…ç½®
HACKERNEWS_API = "https://hacker-news.firebaseio.com/v0"
GITHUB_API = "https://api.github.com"

def fetch_hacker_news(limit: int = 15, min_points: int = 50) -> List[Dict[str, Any]]:
    """æŠ“å– Hacker News çƒ­é—¨æ•…äº‹"""
    try:
        print("ğŸ“Š æ­£åœ¨æŠ“å– Hacker News...")
        url = f"{HACKERNEWS_API}/topstories.json"
        response = requests.get(url, timeout=10)
        story_ids = response.json()[:limit*3]  # è·å–æ›´å¤šä»¥è¿‡æ»¤

        stories = []
        for story_id in story_ids:
            detail_url = f"{HACKERNEWS_API}/item/{story_id}.json"
            detail_response = requests.get(detail_url, timeout=5)
            story = detail_response.json()

            if not story:
                continue

            # è´¨é‡è¿‡æ»¤ï¼šæœ€ä½ç‚¹èµæ•°
            points = story.get('score', 0)
            if points < min_points:
                continue

            # AI/ML ç›¸å…³æ€§è¿‡æ»¤
            title = story.get('title', '').lower()
            url = story.get('url', '')

            ai_keywords = ['ai', 'machine learning', 'deep learning', 'llm', 'gpt',
                         'claude', 'gemini', 'neural', 'robot', 'automation',
                         'model', 'training', 'inference', 'agent']

            if not any(keyword in title for keyword in ai_keywords):
                continue

            story_data = {
                'title': story.get('title', ''),
                'url': url,
                'points': points,
                'comments': story.get('descendants', 0),
                'time': datetime.fromtimestamp(story.get('time', 0)).strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'Hacker News',
                'hn_id': story_id
            }
            stories.append(story_data)

            if len(stories) >= limit:
                break

        print(f"  âœ… è·å– {len(stories)} æ¡ Hacker Newsï¼ˆè¿‡æ»¤åï¼‰")
        return stories

    except Exception as e:
        print(f"  âŒ Hacker News æŠ“å–å¤±è´¥: {e}")
        return []

def fetch_github_trending(limit: int = 10, min_stars: int = 100) -> List[Dict[str, Any]]:
    """æŠ“å– GitHub Trending AI/ML ä»“åº“"""
    try:
        print("ğŸš€ æ­£åœ¨æŠ“å– GitHub Trending...")
        # ä½¿ç”¨æœç´¢ API æŒ‰æœ€è¿‘æ›´æ–°æ’åº
        query = "topic:ai language:python OR language:javascript OR language:typescript"
        url = f"{GITHUB_API}/search/repositories"
        params = {
            'q': query,
            'sort': 'updated',
            'order': 'desc',
            'per_page': limit * 2
        }

        response = requests.get(url, params=params, timeout=10)
        data = response.json()

        repos = []
        for item in data.get('items', []):
            repo = item.get('repository', item)

            # è´¨é‡è¿‡æ»¤ï¼šæœ€ä½ star æ•°
            stars = repo.get('stargazers_count', 0)
            if stars < min_stars:
                continue

            # AI/ML ç›¸å…³æ€§è¿‡æ»¤
            description = repo.get('description', '').lower()
            topics = [t.lower() for t in repo.get('topics', [])]

            ai_keywords = ['ai', 'machine-learning', 'deep-learning', 'llm',
                         'gpt', 'neural-network', 'transformer', 'agent']

            if not any(keyword in description or keyword in str(topics)
                     for keyword in ai_keywords):
                continue

            repo_data = {
                'title': repo.get('full_name', ''),
                'description': repo.get('description', ''),
                'url': repo.get('html_url', ''),
                'stars': stars,
                'language': repo.get('language', 'Unknown'),
                'updated': datetime.strptime(repo.get('updated_at', ''),
                                      '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d %H:%M:%S'),
                'source': 'GitHub Trending',
                'topics': repo.get('topics', [])
            }
            repos.append(repo_data)

            if len(repos) >= limit:
                break

        print(f"  âœ… è·å– {len(repos)} ä¸ª GitHub ä»“åº“ï¼ˆè¿‡æ»¤åï¼‰")
        return repos

    except Exception as e:
        print(f"  âŒ GitHub Trending æŠ“å–å¤±è´¥: {e}")
        return []

def save_to_file(hn_stories: List[Dict], gh_repos: List[Dict]) -> str:
    """ä¿å­˜åˆ°æ–‡ä»¶"""
    # è·å–ç¯å¢ƒå˜é‡
    news_dir = os.environ.get('NEWS_DIR', '/data1/cc/vide-coding/ai-news-hub/docs/news')
    date = os.environ.get('DATE', datetime.now().strftime('%Y-%m-%d'))
    datetime_str = os.environ.get('DATETIME', datetime.now().strftime('%Y-%m-%d %H:%M'))
    output_file = os.path.join(news_dir, f"community-{date}.md")

    # ç”Ÿæˆ Markdown
    content = f"""# {datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')} AI ç¤¾åŒºçƒ­ç‚¹

**æ¥æºï¼š** Hacker News + GitHub Trending
**æ›´æ–°æ—¶é—´ï¼š** {datetime_str}
**æ•°æ®ç»Ÿè®¡ï¼š** {len(hn_stories)} æ¡è®¨è®º + {len(gh_repos)} ä¸ªé¡¹ç›®

---

## ğŸ’¬ Hacker News çƒ­é—¨è®¨è®ºï¼ˆç‚¹èµ >{min_points if hn_stories else 50}ï¼‰

"""

    # Hacker News æ•…äº‹
    for i, story in enumerate(hn_stories, 1):
        content += f"""### {i}. {story['title']}

**ğŸ’¬ ç‚¹èµï¼š** {story['points']} â€¢ **ğŸ’­ è¯„è®ºï¼š** {story['comments']}
**ğŸ”— é“¾æ¥ï¼š** [{story['title']}]({story['url']}) â€¢ [Hacker News è®¨è®º](https://news.ycombinator.com/item?id={story['hn_id']})
**â° æ—¶é—´ï¼š** {story['time']}

---
"""

    # GitHub Trending
    content += """
## ğŸš€ GitHub Trending AI é¡¹ç›®ï¼ˆStar >100ï¼‰

"""

    for i, repo in enumerate(gh_repos, 1):
        topics_str = ', '.join(repo['topics'][:5]) if repo['topics'] else 'N/A'
        content += f"""### {i}. {repo['title']}

**â­ Starsï¼š** {repo['stars']} â€¢ **ğŸ’» è¯­è¨€ï¼š** {repo['language']}
**ğŸ“ æè¿°ï¼š** {repo['description']}
**ğŸ·ï¸ è¯é¢˜ï¼š** {topics_str}
**ğŸ”— é“¾æ¥ï¼š** [{repo['title']}]({repo['url']})
**â° æ›´æ–°ï¼š** {repo['updated']}

---
"""

    content += f"""
---

**æ‰€æœ‰æ•°æ®ç»è¿‡ AI ç›¸å…³æ€§è¿‡æ»¤å’Œè´¨ç†ç­›é€‰**
**æ•°æ®æ¥æºï¼š**
- Hacker News API: https://news.ycombinator.com/
- GitHub API: https://github.com/trending

---

*è‡ªåŠ¨ç”Ÿæˆï¼šAI News Bot | æœ€åæ›´æ–°ï¼š{datetime_str}*
"""

    # ä¿å­˜æ–‡ä»¶
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\nâœ… å·²ä¿å­˜ç¤¾åŒºçƒ­ç‚¹åˆ° {output_file}")
    return output_file

if __name__ == '__main__':
    print(f"\n{'='*60}")
    print("  ç¤¾åŒºå¹³å°æŠ“å–å¼€å§‹ï¼ˆHacker News + GitHubï¼‰")
    print(f"{'='*60}\n")

    # æŠ“å–æ•°æ®
    hn_stories = fetch_hacker_news(limit=10, min_points=50)
    gh_repos = fetch_github_trending(limit=8, min_stars=100)

    # ä¿å­˜æ–‡ä»¶
    if hn_stories or gh_repos:
        save_to_file(hn_stories, gh_repos)
        print(f"{'='*60}\n")
    else:
        print("\nâš ï¸  æœªèƒ½æŠ“å–åˆ°ä»»ä½•æ•°æ®")
