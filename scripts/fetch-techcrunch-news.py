#!/usr/bin/env python3
"""
æŠ“å– TechCrunch AI æ–°é—»å¹¶ä¿å­˜ä¸º Markdown
ä¼˜åŒ–æ ¼å¼ï¼šæ˜¾ç¤ºæ›´ä¸°å¯Œçš„å†…å®¹
"""

import feedparser
import requests
from datetime import datetime
import os
import re

# é…ç½®
RSS_URL = "https://techcrunch.com/category/artificial-intelligence/feed/"

def clean_html(text):
    """æ¸…ç† HTML æ ‡ç­¾"""
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&amp;', '&')
    text = text.replace('&#8217;', "'")
    text = text.replace('&#8216;', "'")
    text = text.replace('&#8220;', '"')
    text = text.replace('&#8221;', '"')
    text = text.replace('&#8230;', '...')
    text = text.replace('&quot;', '"')
    return text.strip()

def fetch_techcrunch_news():
    """æŠ“å– TechCrunch AI æ–°é—»"""
    try:
        feed = feedparser.parse(RSS_URL)
        entries = feed.entries[:15]  # å–å‰15æ¡
        
        # è·å–ç¯å¢ƒå˜é‡
        news_dir = os.environ['NEWS_DIR']
        date = os.environ['DATE']
        datetime_str = os.environ['DATETIME']
        output_file = os.path.join(news_dir, f"{date}.md")
        
        # ç”Ÿæˆå†…å®¹
        content = f"""# {datetime.strptime(date, '%Y-%m-%d').strftime('%Yå¹´%mæœˆ%dæ—¥')} AI æ–°é—»ç®€æŠ¥

**æ¥æºï¼š** TechCrunch AI News RSS
**æ›´æ–°æ—¶é—´ï¼š** {datetime_str}
**æ–°é—»æ•°é‡ï¼š** {len(entries)} æ¡

---

## ğŸ”¥ ä»Šæ—¥å¤´æ¡

"""
        
        # å‰3æ¡æ–°é—»ä½œä¸ºå¤´æ¡
        for i, entry in enumerate(entries[:3], 1):
            title = entry.get('title', 'æ— æ ‡é¢˜')
            link = entry.get('link', '')
            description = entry.get('description', '')
            published = entry.get('published', datetime.now().strftime('%a, %d %b %Y %H:%M:%S %z'))
            author = entry.get('author', 'TechCrunch')
            
            # æ¸…ç† HTML
            description = clean_html(description)
            if len(description) > 400:
                description = description[:400] + '...'
            
            content += f"""
### {i}. {title}

**ğŸ“… å‘å¸ƒæ—¶é—´ï¼š** {published}
**âœï¸ ä½œè€…ï¼š** {author}
**ğŸ”— åŸæ–‡é“¾æ¥ï¼š** [{title}]({link})

**ğŸ“ å†…å®¹æ‘˜è¦ï¼š**
{description}

---
"""
        
        # å…¶ä½™æ–°é—»
        content += """
## ğŸ“° æ›´å¤šèµ„è®¯

"""
        
        for i, entry in enumerate(entries[3:], 4):
            title = entry.get('title', 'æ— æ ‡é¢˜')
            link = entry.get('link', '')
            description = entry.get('description', '')
            published = entry.get('published', datetime.now().strftime('%a, %d %b %Y %H:%M:%S %z'))
            
            # æ¸…ç† HTML
            description = clean_html(description)
            if len(description) > 150:
                description = description[:150] + '...'
            
            content += f"""
### {i}. {title}

**ğŸ“… æ—¶é—´ï¼š** {published}
**ğŸ”— é“¾æ¥ï¼š** [{title}]({link})

{description}

---
"""
        
        content += f"""
## ğŸ“Š æ•°æ®ç»Ÿè®¡

- **æ–°é—»æ¥æºï¼š** TechCrunch AI News
- **æ›´æ–°é¢‘ç‡ï¼š** æ¯æ—¥ 22:00 å’Œ 02:00
- **RSS åœ°å€ï¼š** https://techcrunch.com/category/artificial-intelligence/feed/

---

*è‡ªåŠ¨ç”Ÿæˆï¼šAI News Bot | æœ€åæ›´æ–°ï¼š{datetime_str}*
"""
        
        # ä¿å­˜æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"âœ… å·²ä¿å­˜ {len(entries)} æ¡æ–°é—»åˆ° {output_file}")
        
    except Exception as e:
        print(f"âŒ æŠ“å–æ–°é—»å¤±è´¥: {e}")
        exit(1)

if __name__ == '__main__':
    fetch_techcrunch_news()
