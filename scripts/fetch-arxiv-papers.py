#!/usr/bin/env python3
"""
AI News è®ºæ–‡æŠ“å–è„šæœ¬
æŠ“å– arXiv æœ€æ–°è®ºæ–‡ï¼ŒæŒ‰ç ”ç©¶æ–¹å‘åˆ†ç±»
"""

import feedparser
import requests
from datetime import datetime, timedelta
import json
import re
from pathlib import Path

# é…ç½®
ARXIV_FEEDS = {
    'cs.AI': 'http://export.arxiv.org/rss/cs.AI',
    'cs.CL': 'http://export.arxiv.org/rss/cs.CL',
    'cs.CV': 'http://export.arxiv.org/rss/cs.CV',
    'cs.LG': 'http://export.arxiv.org/rss/cs.LG',
}

OUTPUT_DIR = Path('/data1/cc/vide-coding/ai-news-hub/docs/papers')
DAYS_TO_FETCH = 7  # æŠ“å–æœ€è¿‘7å¤©çš„è®ºæ–‡

def fetch_papers(category='cs.AI', max_papers=20):
    """æŠ“å–æŒ‡å®šåˆ†ç±»çš„æœ€æ–°è®ºæ–‡"""
    feed_url = ARXIV_FEEDS.get(category)
    if not feed_url:
        return []
    
    try:
        feed = feedparser.parse(feed_url)
        papers = []
        
        for entry in feed.entries[:max_papers]:
            # æå–è®ºæ–‡ä¿¡æ¯
            paper = {
                'id': entry.id.split('/abs/')[-1],
                'title': entry.title,
                'authors': [author.name for author in entry.get('authors', [])],
                'summary': entry.get('summary', ''),
                'published': entry.get('published', ''),
                'link': entry.link,
                'pdf_url': entry.link.replace('/abs/', '/pdf/') + '.pdf',
                'category': category,
            }
            papers.append(paper)
        
        return papers
    except Exception as e:
        print(f"Error fetching papers from {category}: {e}")
        return []

def categorize_paper(paper):
    """æ ¹æ®è®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦åˆ†ç±»ç ”ç©¶æ–¹å‘"""
    title = paper['title'].lower()
    summary = paper['summary'].lower()
    
    # è®¡ç®—æœºè§†è§‰ (CV)
    if any(keyword in title or summary for keyword in [
        'vision', 'image', 'visual', 'detection', 'segmentation',
        'recognition', 'generative', 'diffusion', 'gan'
    ]):
        return 'ğŸ–¼ï¸ è®¡ç®—æœºè§†è§‰ (CV)'
    
    # å¼ºåŒ–å­¦ä¹  (RL)
    if any(keyword in title or summary for keyword in [
        'reinforcement', 'rl', 'policy', 'agent', 'reward',
        'offline rl', 'multi-agent'
    ]):
        return 'ğŸ¤– å¼ºåŒ–å­¦ä¹  (RL)'
    
    # å¤§è¯­è¨€æ¨¡å‹ (LLM)
    if any(keyword in title or summary for keyword in [
        'language model', 'llm', 'gpt', 'transformer', 'attention',
        'scaling law', 'inference', 'token', 'embedding'
    ]):
        return 'ğŸ“ å¤§è¯­è¨€æ¨¡å‹ (LLM)'
    
    # å¤šæ¨¡æ€
    if any(keyword in title or summary for keyword in [
        'multimodal', 'vision-language', 'vlm', 'clip',
        'cross-modal', 'fusion'
    ]):
        return 'ğŸ”€ å¤šæ¨¡æ€'
    
    # é»˜è®¤åˆ†ç±»
    return paper['category']

def format_paper_markdown(paper):
    """æ ¼å¼åŒ–è®ºæ–‡ä¸º Markdown"""
    # æå–æ—¥æœŸ
    try:
        pub_date = datetime.strptime(paper['published'], '%a, %d %b %Y %H:%M:%S %z')
        date_str = pub_date.strftime('%Y-%m-%d')
    except:
        date_str = paper['published'][:10]
    
    # åˆ†ç±»
    category_tag = categorize_paper(paper)
    
    # æå–ä½œè€…ï¼ˆæœ€å¤šæ˜¾ç¤º3ä¸ªï¼‰
    authors_str = ', '.join(paper['authors'][:3])
    if len(paper['authors']) > 3:
        authors_str += f" et al. ({len(paper['authors'])} authors)"
    
    markdown = f"""### {paper['title']}

**ä½œè€…ï¼š** {authors_str}  
**æ—¶é—´ï¼š** {date_str}  
**åˆ†ç±»ï¼š** {category_tag}

**æ‘˜è¦ï¼š** {paper['summary'][:200]}...

**åŸæ–‡é“¾æ¥ï¼š** [{paper['id']}]({paper['link']})

**PDFï¼š** [ä¸‹è½½]({paper['pdf_url']})

---

"""
    return markdown

def save_papers_by_date(papers, date_str):
    """ä¿å­˜è®ºæ–‡åˆ°æŒ‡å®šæ—¥æœŸçš„æ–‡ä»¶"""
    output_file = OUTPUT_DIR / f"{date_str}.md"
    
    # æŒ‰åˆ†ç±»ç»„ç»‡
    papers_by_category = {}
    for paper in papers:
        category = categorize_paper(paper)
        if category not in papers_by_category:
            papers_by_category[category] = []
        papers_by_category[category].append(paper)
    
    # ç”Ÿæˆ Markdown
    markdown_content = f"""# {date_str} å¹´{datetime.strptime(date_str, '%Y-%m-%d').strftime('%mæœˆ%dæ—¥')} AI è®ºæ–‡

**æ¥æºï¼š** arXiv.org  
**æ›´æ–°æ—¶é—´ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M')}

---

"""
    
    # æ·»åŠ å„åˆ†ç±»çš„è®ºæ–‡
    for category in sorted(papers_by_category.keys()):
        markdown_content += f"\n## {category}\n\n"
        
        category_papers = papers_by_category[category]
        for i, paper in enumerate(category_papers, 1):
            markdown_content += format_paper_markdown(paper)
            markdown_content += "\n"
    
    # ä¿å­˜æ–‡ä»¶
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    print(f"âœ… å·²ä¿å­˜ {len(papers)} ç¯‡è®ºæ–‡åˆ° {output_file}")
    return output_file

def main():
    """ä¸»å‡½æ•°"""
    date_str = (datetime.now() - timedelta(days=0)).strftime('%Y-%m-%d')
    
    print(f"ğŸ“š å¼€å§‹æŠ“å– arXiv è®ºæ–‡...")
    print(f"ğŸ“… æ—¥æœŸï¼š{date_str}")
    
    # æŠ“å–å„åˆ†ç±»è®ºæ–‡
    all_papers = []
    for category in ARXIV_FEEDS.keys():
        print(f"ğŸ” æŠ“å– {category} è®ºæ–‡...")
        papers = fetch_papers(category, max_papers=10)
        all_papers.extend(papers)
        print(f"  âœ… è·å– {len(papers)} ç¯‡")
    
    # å»é‡ï¼ˆåŸºäº IDï¼‰
    seen_ids = set()
    unique_papers = []
    for paper in all_papers:
        if paper['id'] not in seen_ids:
            unique_papers.append(paper)
            seen_ids.add(paper['id'])
    
    print(f"\nğŸ“Š æ€»è®¡è·å– {len(unique_papers)} ç¯‡å”¯ä¸€è®ºæ–‡")
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    if unique_papers:
        output_file = save_papers_by_date(unique_papers, date_str)
        print(f"\nâœ… è®ºæ–‡æŠ“å–å®Œæˆï¼")
        print(f"ğŸ“„ æ–‡ä»¶ï¼š{output_file}")
    else:
        print("\nâš ï¸  æ²¡æœ‰è·å–åˆ°è®ºæ–‡")

if __name__ == '__main__':
    main()
